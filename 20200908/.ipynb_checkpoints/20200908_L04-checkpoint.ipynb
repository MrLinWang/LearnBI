{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心能力提升班商业智能方向 004期 Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 1： ALS都有哪些应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS（Alternating Least Squares）交替最小二乘法，通常用于最优化矩阵分解问题，也可以应用在线性回归问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 2： ALS进行矩阵分解的时候，为什么可以并行化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS在每次迭代时，固定用户因子矩阵或者是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，知道模型收敛（或者是迭代了预设好的次数）。因此在进行分解的时候可以分开计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 3： 梯度下降法中的批量梯度下降（BGD），随机梯度下降（SGD），和小批量梯度下降有什么区别（MBGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量梯度下降（BGD）在每次迭代时会使用所有样本进行梯度计算，这样的优点是可以计算全域梯度，使得参数更新的方向更靠近目标值，但缺点是计算量很大，训练过程太慢。  \n",
    "随机梯度下降（SGD）在每次迭代时会使用一个样本进行梯度计算，这样的优点是计算速度很快，但缺点是损失震荡较大，并且可能无法收敛到全局最优。\n",
    "小批量梯度下降（MBGD）是一个折中办法，每次迭代使用mini_batch_size个样本进行梯度计算，这样的优点是计算速度相较BGD快了很多，同时也能够使得收敛效果更接近BGD的效果，其缺点是需要人为设定mini_batch_size的大小，如果设置不当就无法发挥出MBGD的优势了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 4： 你阅读过和推荐系统/计算广告/预测相关的论文么？有哪些论文是你比较推荐的，可以分享到微信群中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我主要阅读的是基于深度学习的推荐系统相关论文，比较推荐的是Alibaba的几篇顶会论文，是非常有行业引领性的：   \n",
    "1. Zhou, Guorui & Song, Chengru & Zhu, Xiaoqiang & Ma, Xiao & Yan, Yanghui & Dai, Xingya & Zhu, Han & Jin, Junqi & Li, Han & Gai, Kun. (2017). Deep Interest Network for Click-Through Rate Prediction. \n",
    "2. Zhou, Guorui & Mou, Na & Fan, Ying & Pi, Qi & Bian, Weijie & Zhou, Chang & Zhu, Xiaoqiang & Gai, Kun. (2018). Deep Interest Evolution Network for Click-Through Rate Prediction. \n",
    "3. Chen, Qiwei & Zhao, Huan & Li, Wei & Huang, Pipei & Ou, Wenwu. (2019). Behavior Sequence Transformer for E-commerce Recommendation in Alibaba. \n",
    "4. Sun, Fei & Liu, Jun & Wu, Jian & Pei, Changhua & Lin, Xiao & Ou, Wenwu & Jiang, Peng. (2019). BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 1： 对MovieLens数据集进行评分预测\n",
    "工具：可以使用Surprise或者其他  \n",
    "说明使用的模型，及简要原理  \n",
    "数据集：MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import BaselineOnly, SlopeOne\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据读取\n",
    "reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
    "data = Dataset.load_from_file('data/ratings.csv', reader=reader)\n",
    "train_set = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BaselineOnly算法\n",
    "分别应用了ALS优化方法和SGD优化方法  \n",
    "$$ \\hat{r_{ui}} = b_u + b_i + \\mu  $$\n",
    "Baseline算法是建立整体基线，然后根据user偏差和item偏差来得到评分。  \n",
    "其中$\\mu$可以通过统计得到，而$b_u$和$b_i$可以通过ALS或SGD得到。  \n",
    "<b>ALS</b>中文名为交替最小二乘法，是求解矩阵分解问题的一种最优化方法，其原理是迭代式求解一系列最小二乘回归问题。在每次迭代时，固定用户因子矩阵或者是物品因子矩阵中的一个，然后用固定的这个矩阵以及评级数据来更新另一个矩阵。之后，被更新的矩阵被固定住，再更新另外一个矩阵。如此迭代，知道模型收敛（或者是迭代了预设好的次数）。  \n",
    "<b>SGD</b>中文名为随机梯度下降，是一种常用的最优化方法，其原理是迭代的进行参数更新，每次迭代只用一个训练数据进行梯度计算，这样带来的优势是计算速度快，但缺点是在训练过程中会造成损失震荡幅度较大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALS优化\n",
    "bsl_options = {'method': 'als','n_epochs': 5,'reg_u': 12,'reg_i': 5}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 0.8651\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8643\n",
      "Estimating biases using als...\n",
      "RMSE: 0.8618\n"
     ]
    }
   ],
   "source": [
    "# 定义K折交叉验证迭代器，K=3\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset, testset in kf.split(data):\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 196        item: 302        r_ui = 4.00   est = 4.14   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "uid = str(196)\n",
    "iid = str(302)\n",
    "# 输出uid对iid的预测结果\n",
    "pred = algo.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "RMSE: 0.8754\n",
      "Estimating biases using sgd...\n",
      "RMSE: 0.8732\n",
      "Estimating biases using sgd...\n",
      "RMSE: 0.8741\n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.00   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# SGD优化\n",
    "bsl_options = {'method': 'sgd','n_epochs': 5}\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "# 定义K折交叉验证迭代器，K=3\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset, testset in kf.split(data):\n",
    "    # 训练并预测\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)\n",
    "uid = str(196)\n",
    "iid = str(302)\n",
    "# 输出uid对iid的预测结果\n",
    "pred = algo.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SlopeOne算法\n",
    "SlopeOne算法基于user之间和item之间的评分差异来进行评分预测的。其大致分为三步：\n",
    "1. 计算物品之间的评分差的均值，记为物品间的评分偏差(两物品同时被评分)\n",
    "2. 根据物品间的评分偏差和用户的历史评分，预测用户对未评分的物品的评分\n",
    "3. 将预测评分排序，取topN对应的物品推荐给用户"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8671\n",
      "RMSE: 0.8677\n",
      "RMSE: 0.8679\n"
     ]
    }
   ],
   "source": [
    "# 使用SlopeOne算法\n",
    "algo2 = SlopeOne()\n",
    "# 定义K折交叉验证迭代器，K=3\n",
    "kf = KFold(n_splits=3)\n",
    "for trainset, testset in kf.split(data):\n",
    "    # 训练并预测\n",
    "    algo2.fit(trainset)\n",
    "    predictions = algo2.test(testset)\n",
    "    # 计算RMSE\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 196        item: 302        r_ui = 4.00   est = 4.31   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# 输出uid对iid的预测结果\n",
    "pred = algo2.predict(uid, iid, r_ui=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 2： Paper Reading：Slope one predictors for online rating-based collaborative filtering. Daniel Lemire and Anna Maclachlan, 2007. http://arxiv.org/abs/cs/0702144.\n",
    "积累，总结笔记，自己的思考及idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Introduction\n",
    "论文作者提出鲁棒性的CF应该符合以下几点：\n",
    "1. 算法容易实现和维护\n",
    "2. 对新的评分应该立即给予响应\n",
    "3. 查询速度要快\n",
    "4. 对新的用户也要能给出有效的推荐\n",
    "5. 在不做出重大牺牲的前提下精度上要有竞争力\n",
    "\n",
    "Slope One的核心思想是利用用户之间的评分差异和商品之间的人气差异（popularity differential）  \n",
    "<img src=\"Figure 1.png\">  \n",
    "此文章的主要贡献是提出了Slope One协同过滤方法，并且展示了其相比memory-based的方法在CF任务上具有几乎相同的精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Related Work\n",
    "主要介绍基于内存的和基于模型的几种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 CF Algorithms\n",
    "首先引出了作为对比的四个其他方案： <b>PER USER AVERAGE, BIAS FROM MEAN, ADJUSTED COSINE ITEM-BASED and The PEARSON Reference Scheme</b> \n",
    "1. PER USER AVERAGE: 一个简单粗暴的方案，直接将user对所有item的评分的平均值作为user对于新item的评分。\n",
    "$$P(u) = \\bar u $$\n",
    "2. BIAS FROM MEAN：在user评分均值的基础上加入所有user对item的评分偏差。  \n",
    "$$ P(u)_{i} = \\bar{u} + \\frac{1}{card(S_{i}(\\chi ))}\\sum_{\\nu\\in S_{i}(\\chi )}\\nu_{i}-\\bar{\\nu} $$   \n",
    "其中，$P(u)_{i}$是用户u对商品i的评分， $ S_{i}(\\chi )$是有对$i$作过评分的用户的集合，$\\nu_{i}$是用户$v$对商品$i$的评分，$card$是其内集合的元素项的个数。\n",
    "3. ADJUSTED COSINE ITEM-BASED：通过user对item的评分来计算item间的相似度，从而推荐相似的商品给用户。\n",
    "$$sim_{i,j} = \\frac{\\sum_{u\\in S_{i,j}(\\chi )}(u_{i}-\\bar{u})(u_{j}-\\bar{u})}{\\sqrt{\\sum_{u\\in S_{i,j}(\\chi )}(u_{i}-\\bar{u})^{2}\\sum_{u\\in S_{i,j}(\\chi )}(u_{j}-\\bar{u})^{2}}}$$\n",
    "$$P(u)_{i} = \\frac{\\sum _{j\\in S(u)}\\left | sim_{i,j} \\right | (\\alpha _{i,j}u_{j}+\\beta _{i,j})}{\\sum _{j\\in S(u)}\\left | sim_{i,j} \\right |}$$\n",
    "其中，$S_{i,j}(\\chi )$是有对$i$,$j$都作过评分的用户的集合，$sim_{i,j}$计算商品$i$,$j$间的相似度，$P(u)_{i}$即用户u对商品i的评分，$\\alpha$和$\\beta$是校正因子。\n",
    "4. The PEARSON Reference Scheme：是一个基于内存的算法，在BIAS FROM MEAN算法的基础上引入了user间的相似度作为权重。\n",
    "$$Corr(u,w)=\\frac{< u-\\bar u,w-\\bar w > }{\\sqrt{\\sum_{i\\in S(u)\\bigcap S(w)}(u_{i}-\\bar{u})^{2}\\sum_{i\\in S(u)\\bigcap S(w)}(w_{i}-\\bar{w})^{2}}}$$\n",
    "$$\\gamma (u,w) = Corr(u,w)\\left | Corr(u,w) \\right |^{\\rho -1}$$\n",
    "$$P(u)_{i} = \\bar{u} + \\frac{\\sum _{\\nu \\in S_i (\\chi )}\\gamma (u,\\nu )(\\nu _i - \\bar \\nu)}{\\sum _{\\nu \\in S_i (\\chi )}\\left | \\gamma (u,\\nu ) \\right |}$$\n",
    "其中，$\\gamma(u,\\nu)$ 是用户$u$,$\\nu$之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文提出了三个方案：<b>The SLOPE ONE Scheme, The WEIGHTED SLOPE ONE Scheme, The BI-POLAR SLOPE ONE Scheme</b>\n",
    "1. The SLOPE ONE Scheme：用item之间的评分差异的均值来代表item之间的差别，用user之间的评分差异的均值来代表user之间的差别，根据user之间的差异和item之间的差异来计算待求评分。\n",
    "$$dev_{j,i} = \\sum_{u\\in S_{j,i}(\\chi )}\\frac{u_j - u_i}{card(S_{j,i}(\\chi ))}$$\n",
    "$$P(u)_j = \\frac{1}{card(R_j)}\\sum_{i\\in R_j}(dev_{j,i}+u_i)$$\n",
    "$$P^{S1}(u)_j = \\bar u + \\frac{1}{card(R_j)}dev_{j,i}$$\n",
    "其中，$dev_{j,i}$表示item i,j之间的评分差异，$P(u)_j$通过平均不同商品的评分差距可以得到user i对item j的评分。\n",
    "2. The WEIGHTED SLOPE ONE Scheme：Slope One中对于不同的评分是一视同仁的，但事实上不同的评分的可信度不同，通过引入权重提高可信的评分的影响（被评分较多的item的评分可信度较高）。\n",
    "$$P^{wS1}(u)_j = \\frac{\\sum_{i \\in S(u)-\\left \\{ j \\right \\}}(dev_{j,i}+u_i)c_{j,i}}{\\sum_{i \\in S(u)-\\left \\{ j \\right \\}}c_{j,i}}$$\n",
    "其中，$c_{j,i} = card(S_{j,i}(\\chi ))$\n",
    "3. The BI-POLAR SLOPE ONE Scheme：从user对item i,j的评分中筛选出喜欢item i,j的评分和不喜欢item i,j的评分（其中喜欢和不喜欢的定义是评分高于或低于user的平均评分），只用这些评分来计算item i,j之间的评分差异，这样减少了个人喜好对于评分的影响。\n",
    "$$P^{bpS1}(u)_{j} = \\frac{\\sum_{i \\in S^{like}(u)-\\left \\{ j \\right \\}}p_{j,i}^{like}c_{j,i}^{like}+\\sum_{i \\in S^{diskile}(u)-\\left \\{ j \\right \\}}p_{j,i}^{dislike}c_{j,i}^{dislike}}{\\sum_{i \\in S^{like}(u)-\\left \\{ j \\right \\}}c_{j,i}^{like}+\\sum_{i \\in S^{dislike}(u)-\\left \\{ j \\right \\}}c_{j,i}^{dislike}}$$\n",
    "其中，$c_{j,i}^{like} = card(S_{j,i}^{like})$, $c_{j,i}^{dislike} = card(S_{j,i}^{dislike})$, $S^{like}(u) = \\left \\{ i\\in S(u)|u_{i}>\\bar{u} \\right \\}$, $S^{dislike}(u) = \\left \\{ i\\in S(u)|u_{i}<\\bar{u} \\right \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 Experimental Results\n",
    "<img src=\"Table 1.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>自己的思考</b>\n",
    "Slope One算法的核心思想是计算user之间和item之间的区别来“类比”的推演出待评分的商品的评分，其中BI-POLAR SLOPE ONE算法通过筛选方式减少个人喜好对于评分的影响，所以在此基础上也应该可以应用K-Means算法先将user和item进行聚类，然后分别计算同类之间的评分差距（就像BI-POLAR SLOPE ONE中分别计算like和dislike那样），这样可以可以进一步减少user或item类别差异对评分差距造成的偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 3： 设计你自己的句子生成器\n",
    "grammar = '''  \n",
    "战斗 => 施法  ， 结果 。  \n",
    "施法 => 主语 动作 技能   \n",
    "结果 => 主语 获得 效果  \n",
    "主语 => 张飞 | 关羽 | 赵云 | 典韦 | 许褚 | 刘备 | 黄忠 | 曹操 | 鲁班七号 | 貂蝉  \n",
    "动作 => 施放 | 使用 | 召唤   \n",
    "技能 => 一骑当千 | 单刀赴会 | 青龙偃月 | 刀锋铁骑 | 黑暗潜能 | 画地为牢 | 守护机关 | 狂兽血性 | 龙鸣 | 惊雷之龙 | 破云之龙 | 天翔之龙\n",
    "获得 => 损失 | 获得   \n",
    "效果 => 数值 状态  \n",
    "数值 => 1 | 1000 |5000 | 100   \n",
    "状态 => 法力 | 生命  \n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 定语从句语法\n",
    "grammar = '''\n",
    "战斗 => 施法  ， 结果 。\n",
    "施法 => 主语 动作 技能 \n",
    "结果 => 主语 获得 效果\n",
    "主语 => 张飞 | 关羽 | 赵云 | 典韦 | 许褚 | 刘备 | 黄忠 | 曹操 | 鲁班七号 | 貂蝉\n",
    "动作 => 施放 | 使用 | 召唤 \n",
    "技能 => 一骑当千 | 单刀赴会 | 青龙偃月 | 刀锋铁骑 | 黑暗潜能 | 画地为牢 | 守护机关 | 狂兽血性 | 龙鸣 | 惊雷之龙 | 破云之龙 | 天翔之龙\n",
    "获得 => 损失 | 获得 \n",
    "效果 => 数值 状态\n",
    "数值 => 1 | 1000 |5000 | 100 \n",
    "状态 => 法力 | 生命\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到语法字典\n",
    "def getGrammarDict(gram, linesplit = \"\\n\", gramsplit = \"=>\"):\n",
    "    #定义字典\n",
    "    result = {}\n",
    "\n",
    "    for line in gram.split(linesplit):\n",
    "        # 去掉首尾空格后，如果为空则退出\n",
    "        if not line.strip(): \n",
    "            continue\n",
    "        expr, statement = line.split(gramsplit)\n",
    "        result[expr.strip()] = [i.split() for i in statement.split(\"|\")]\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'战斗': [['施法', '，', '结果', '。']],\n",
       " '施法': [['主语', '动作', '技能']],\n",
       " '结果': [['主语', '获得', '效果']],\n",
       " '主语': [['张飞'],\n",
       "  ['关羽'],\n",
       "  ['赵云'],\n",
       "  ['典韦'],\n",
       "  ['许褚'],\n",
       "  ['刘备'],\n",
       "  ['黄忠'],\n",
       "  ['曹操'],\n",
       "  ['鲁班七号'],\n",
       "  ['貂蝉']],\n",
       " '动作': [['施放'], ['使用'], ['召唤']],\n",
       " '技能': [['一骑当千'],\n",
       "  ['单刀赴会'],\n",
       "  ['青龙偃月'],\n",
       "  ['刀锋铁骑'],\n",
       "  ['黑暗潜能'],\n",
       "  ['画地为牢'],\n",
       "  ['守护机关'],\n",
       "  ['狂兽血性'],\n",
       "  ['龙鸣'],\n",
       "  ['惊雷之龙'],\n",
       "  ['破云之龙'],\n",
       "  ['天翔之龙']],\n",
       " '获得': [['损失'], ['获得']],\n",
       " '效果': [['数值', '状态']],\n",
       " '数值': [['1'], ['1000'], ['5000'], ['100']],\n",
       " '状态': [['法力'], ['生命']]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gramdict = getGrammarDict(grammar)\n",
    "gramdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成句子\n",
    "def generate(gramdict, target, isEng = False):\n",
    "    if target not in gramdict: \n",
    "        return target\n",
    "    find = random.choice(gramdict[target])\n",
    "    #print(find)\n",
    "    blank = ''\n",
    "    # 如果是英文中间间隔为空格\n",
    "    if isEng: \n",
    "        blank = ' '\n",
    "    return blank.join(generate(gramdict, t, isEng) for t in find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "曹操召唤天翔之龙，貂蝉损失1法力。\n",
      "许褚 施放 破云之龙 ， 张飞 损失 1000 生命 。\n"
     ]
    }
   ],
   "source": [
    "print(generate(gramdict,\"战斗\"))\n",
    "print(generate(gramdict,\"战斗\", True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 具体业务 结尾 \n",
    "报数 = 我是工号 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'host': [['寒暄', '报数', '询问', '具体业务', '结尾']],\n",
       " '报数': [['我是工号', '数字', '号', ',']],\n",
       " '数字': [['单个数字'], ['数字', '单个数字']],\n",
       " '单个数字': [['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']],\n",
       " '寒暄': [['称谓', '打招呼'], ['打招呼']],\n",
       " '称谓': [['人称', ',']],\n",
       " '人称': [['先生'], ['女士'], ['小朋友']],\n",
       " '打招呼': [['你好'], ['您好']],\n",
       " '询问': [['请问你要'], ['您需要']],\n",
       " '具体业务': [['喝酒'], ['打牌'], ['打猎'], ['赌博']],\n",
       " '结尾': [['吗？']]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostdict = getGrammarDict(host, linesplit = \"\\n\", gramsplit = \"=\")\n",
    "hostdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好我是工号25号,您需要喝酒吗？\n",
      "小朋友,您好我是工号7号,请问你要打猎吗？\n"
     ]
    }
   ],
   "source": [
    "print(generate(hostdict,\"host\"))\n",
    "print(generate(hostdict,\"host\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_BI",
   "language": "python",
   "name": "learn_bi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
