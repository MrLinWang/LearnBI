{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心能力提升班商业智能方向 004期 Week 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 1: 常用的文本分类方法都有哪些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 朴素贝叶斯分类（Naïve Bayes）：朴素贝叶斯分类器基于属性条件独立假设：对于已知类别，假设所有属性相互独立，也就是说，假设每个属性独立地对分类结果产生影响。\n",
    "2. KNN文本分类算法：KNN法由Cover和Hart于1968年提出，是一个理论上比较成熟的方法。该算法的基本思想是:根据传统的向量空间模型，文本内容被形式化为特征空间中的加权特征向量。对于一个测试文本，计算它与训练样本集中每个文本的相似度，找出K个最相似的文本，根据加权距离和判断测试文本所属的类别，具体算法步骤如下:\n",
    "   a)对于一个测试文本，根据特征词形成测试文本向量。\n",
    "   b)计算该测试文本与训练集中每个文本的文本相似度，按照文本相似度，在训练文本集中选出与测试文本最相似的k个文本。\n",
    "   c)在测试文本的k个近邻中，依次计算每类的权重。\n",
    "   d)比较类的权重，将文本分到权重最大的那个类别中。\n",
    "3. 支持向量机（SVM）算法：是一种建立在统计学习理论基础上的机器学习方法。该算法基于结构风险最小化原理，将数据集合压缩到支持向量集合，学习得到分类决策函数。\n",
    "4. fastText：其核心思想是将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。\n",
    "5. TextCNN：在文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。\n",
    "6. TextRNN: 递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 \"n-gram\" 信息。\n",
    "7. Bert: 通过Transformer捕捉语句中的双向关系。使用了Mask Language Model(MLM)和 Next Sentence Prediction(NSP) 的多任务预训练模型。使用海量语料数据进行预训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 2: RNN为什么会出现梯度消失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看到网上一篇文章的解释很好，所以摘抄了下来：https://www.cnblogs.com/jins-note/p/10853788.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经典的RNN结构如下图所示：  \n",
    "<img src=\"./RNN.png\"></img>  \n",
    "假设我们的时间序列只有三段， $S_{0}$ 为给定值，神经元没有激活函数，则RNN最简单的前向传播过程如下：\n",
    "$$S_{1}=W_{x}X_{1}+W_{s}S_{0}+b_{1}O_{1}=W_{o}S_{1}+b_{2}$$\n",
    "$$S_{2}=W_{x}X_{2}+W_{s}S_{1}+b_{1}O_{2}=W_{o}S_{2}+b_{2}$$\n",
    "$$S_{3}=W_{x}X_{3}+W_{s}S_{2}+b_{1}O_{3}=W_{o}S_{3}+b_{2}$$\n",
    "假设在t=3时刻，损失函数为 $L_{3}=\\frac{1}{2}(Y_{3}-O_{3})^{2} $。\n",
    "\n",
    "则对于一次训练任务的损失函数为 $L=\\sum_{t=0}^{T}{L_{t}} $，即每一时刻损失值的累加。\n",
    "\n",
    "使用随机梯度下降法训练RNN其实就是对 $W_{x} $、$ W_{s}$ 、 $W_{o} $以及$ b_{1}b_{2}$ 求偏导，并不断调整它们以使L尽可能达到最小的过程。\n",
    "\n",
    "现在假设我们我们的时间序列只有三段，$t1$，$t2$，$t3$。\n",
    "\n",
    "我们只对t3时刻的 $W_{x}$、$W_{s}$、$W_{0}$ 求偏导（其他时刻类似）：\n",
    "\n",
    "$$\\frac{\\partial{L_{3}}}{\\partial{W_{0}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{W_{o}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L_{3}}}{\\partial{W_{x}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{x}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{x}}}$$\n",
    "\n",
    "$$\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}$$\n",
    "\n",
    "可以看出对于 $W_{0}$ 求偏导并没有长期依赖，但是对于 $W_{x}$、$W_{s}$ 求偏导，会随着时间序列产生长期依赖。因为 $S_{t}$ 随着时间序列向前传播，而 $S_{t}$ 又是 $W_{x}$、$W_{s}$的函数。\n",
    "\n",
    "根据上述求偏导的过程，我们可以得出任意时刻对 $W_{x}$、$W_{s}$ 求偏导的公式：\n",
    "\n",
    "$$\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}}$$\n",
    "\n",
    "任意时刻对$W_{s}$ 求偏导的公式同上。\n",
    "\n",
    "如果加上激活函数， $S_{j}=tanh(W_{x}X_{j}+W_{s}S_{j-1}+b_{1})$ ，\n",
    "\n",
    "则 $\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}} = \\prod_{j=k+1}^{t}{tanh^{'}}W_{s}$\n",
    "激活函数tanh和它的导数图像如下。  \n",
    "<img src=\"./tanh.png\"></img>  \n",
    "由上图可以看出 $tanh^{'}\\leq1 $，对于训练过程大部分情况下$tanh$的导数是小于1的，因为很少情况下会出现$W_{x}X_{j}+W_{s}S_{j-1}+b_{1}=0$ ，如果 $W_{s}$ 也是一个大于0小于1的值，则当t很大时 $\\prod_{j=k+1}^{t}{tanh^{'}}W_{s} $，就会趋近于0，和 $0.01^{50}$ 趋近与0是一个道理。同理当 $W_{s} $很大时$ \\prod_{j=k+1}^{t}{tanh^{'}}W_{s} $就会趋近于无穷，这就是RNN中梯度消失和爆炸的原因。\n",
    "\n",
    "至于怎么避免这种现象，再看看 $\\frac{\\partial{L_{t}}}{\\partial{W_{x}}}=\\sum_{k=0}^{t}{\\frac{\\partial{L_{t}}}{\\partial{O_{t}}}\\frac{\\partial{O_{t}}}{\\partial{S_{t}}}}(\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}})\\frac{\\partial{S_{k}}}{\\partial{W_{x}}} $梯度消失和爆炸的根本原因就是 $\\prod_{j=k+1}^{t}{\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}$ 这一坨，要消除这种情况就需要把这一坨在求偏导的过程中去掉，至于怎么去掉，一种办法就是使$ {\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx1 $另一种办法就是使 ${\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}}\\approx0 $。其实这就是LSTM做的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM如何解决梯度消失问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先上一张LSTM的经典图：  \n",
    "<img src=\"./lstm.png\"></img>  \n",
    "RNN梯度消失和爆炸的原因这篇文章中提到的RNN结构可以抽象成下面这幅图：  \n",
    "<img src=\"./rnn_reson.png\"></img>   \n",
    "而LSTM可以抽象成这样：   \n",
    "<img src=\"./lstm_reson.png\"></img>    \n",
    "三个×分别代表的就是forget gate，input gate，output gate，而我认为LSTM最关键的就是forget gate这个部件。这三个gate是如何控制流入流出的呢，其实就是通过下面 $f_{t},i_{t},o_{t}$ 三个函数来控制，因为$ \\sigma(x)$（代表sigmoid函数） 的值是介于0到1之间的，刚好用趋近于0时表示流入不能通过gate，趋近于1时表示流入可以通过gate。\n",
    "\n",
    "$$f_{t}=\\sigma({W_{f}X_{t}}+b_{f})$$\n",
    "\n",
    "$$i_{t}=\\sigma({W_{i}X_{t}}+b_{i})$$\n",
    "\n",
    "$$o_{i}=\\sigma({W_{o}X_{t}}+b_{o})$$\n",
    "\n",
    "当前的状态 $S_{t}=f_{t}S_{t-1}+i_{t}X_{t}$类似与传统RNN$ S_{t}=W_{s}S_{t-1}+W_{x}X_{t}+b_{1}$。  \n",
    "将LSTM的状态表达式展开后得：\n",
    "\n",
    "$$S_{t}=\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}$$\n",
    "\n",
    "如果加上激活函数， $S_{t}=tanh\\left[\\sigma(W_{f}X_{t}+b_{f})S_{t-1}+\\sigma(W_{i}X_{t}+b_{i})X_{t}\\right]$\n",
    "\n",
    "RNN梯度消失和爆炸的原因这篇文章中传统RNN求偏导的过程包含 $$\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{'}W_{s}}$$\n",
    "\n",
    "对于LSTM同样也包含这样的一项，但是在LSTM中 $\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}$\n",
    "\n",
    "假设 $Z=tanh{'}(x)\\sigma({y})$ ，则 Z 的函数图像如下图所示：  \n",
    "<img src=\"./zed.png\"></img>  \n",
    "可以看到该函数值基本上不是0就是1。\n",
    "\n",
    "传统RNN的求偏导过程：\n",
    "\n",
    "$$\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{S_{2}}}\\frac{\\partial{S_{2}}}{\\partial{S_{1}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}$$\n",
    "\n",
    "如果在LSTM中上式可能就会变成：\n",
    "\n",
    "$$\\frac{\\partial{L_{3}}}{\\partial{W_{s}}}=\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{3}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{2}}}{\\partial{W_{s}}}+\\frac{\\partial{L_{3}}}{\\partial{O_{3}}}\\frac{\\partial{O_{3}}}{\\partial{S_{3}}}\\frac{\\partial{S_{1}}}{\\partial{W_{s}}}$$\n",
    "\n",
    "因为 $\\prod_{j=k+1}^{t}\\frac{\\partial{S_{j}}}{\\partial{S_{j-1}}}=\\prod_{j=k+1}^{t}{tanh{’}\\sigma({W_{f}X_{t}+b_{f}})}\\approx0|1$ ，这样就解决了传统RNN中梯度消失的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 1: cnews 中文文本分类：\n",
    "由清华大学根据新浪新闻RSS订阅频道2005-2011年间的历史数据筛选过滤生成  \n",
    "训练集 50000  \n",
    "验证集 5000  \n",
    "测试集 10000  \n",
    "词汇（字） 5000  \n",
    "10个分类，包括：'体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
      "x_train= [[1609  659   56 ...    9  311    3]\n",
      " [   2  101   16 ... 1168    3   24]\n",
      " [ 465  855  521 ...  116  136   85]\n",
      " ...\n",
      " [  49   18   79 ...  836 1928 1072]\n",
      " [ 166  110  714 ...  836 1928 1072]\n",
      " [   1   80  551 ...   78  192    3]]\n"
     ]
    }
   ],
   "source": [
    "# 引包\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from model import TextRNN\n",
    "from cnews_loader import read_vocab, read_category, process_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置数据目标\n",
    "train_file = 'cnews.train.txt'\n",
    "test_file = 'cnews.test.txt'\n",
    "val_file = 'cnews.val.txt'\n",
    "vocab_file = 'cnews.vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Train_Epoch):\n",
    "    model = TextRNN().cuda()\n",
    "    # 定义损失函数\n",
    "    Loss = nn.MultiLabelSoftMarginLoss()\n",
    "    # 定义优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    # 训练\n",
    "    for epoch in range(Train_Epoch):\n",
    "        print('epoch=', epoch)\n",
    "        # 分批训练\n",
    "        for step, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x = x_batch.cuda()\n",
    "            y = y_batch.cuda()\n",
    "            out = model(x)\n",
    "            loss = Loss(out, y)\n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # 计算准确率\n",
    "            accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "        print('train loss=', loss)\n",
    "        print('train accuracy:', accuracy)\n",
    "        # 对模型进行验证\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            for step, (x_batch, y_batch) in enumerate(val_loader):\n",
    "                x = x_batch.cuda()\n",
    "                y = y_batch.cuda()\n",
    "                out = model(x)\n",
    "                accuracy = np.mean((torch.argmax(out, 1) == torch.argmax(y, 1)).cpu().numpy())\n",
    "                if accuracy > best_val_acc:\n",
    "                    torch.save(model, \"model.pkl\")\n",
    "                    best_val_acc = accuracy\n",
    "                    print('model.pkl saved')\n",
    "                    print('val accuracy:', accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取文本的类别及其对应id的字典\n",
    "categories, cat_to_id = read_category()\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '，',\n",
       " '的',\n",
       " '。',\n",
       " '一',\n",
       " '是',\n",
       " '在',\n",
       " '0',\n",
       " '有',\n",
       " '不',\n",
       " '了',\n",
       " '中',\n",
       " '1',\n",
       " '人',\n",
       " '大',\n",
       " '、',\n",
       " '国',\n",
       " '',\n",
       " '2',\n",
       " '这',\n",
       " '上',\n",
       " '为',\n",
       " '个',\n",
       " '“',\n",
       " '”',\n",
       " '年',\n",
       " '学',\n",
       " '时',\n",
       " '我',\n",
       " '地',\n",
       " '和',\n",
       " '以',\n",
       " '到',\n",
       " '出',\n",
       " '来',\n",
       " '会',\n",
       " '行',\n",
       " '发',\n",
       " '：',\n",
       " '对',\n",
       " '们',\n",
       " '要',\n",
       " '生',\n",
       " '家',\n",
       " '他',\n",
       " '能',\n",
       " '也',\n",
       " '业',\n",
       " '金',\n",
       " '3',\n",
       " '成',\n",
       " '可',\n",
       " '分',\n",
       " '多',\n",
       " '现',\n",
       " '5',\n",
       " '就',\n",
       " '场',\n",
       " '新',\n",
       " '后',\n",
       " '于',\n",
       " '下',\n",
       " '日',\n",
       " '经',\n",
       " '市',\n",
       " '前',\n",
       " '过',\n",
       " '方',\n",
       " '得',\n",
       " '作',\n",
       " '月',\n",
       " '最',\n",
       " '开',\n",
       " '房',\n",
       " '》',\n",
       " '《',\n",
       " '高',\n",
       " '9',\n",
       " '8',\n",
       " '.',\n",
       " '而',\n",
       " '比',\n",
       " '公',\n",
       " '4',\n",
       " '说',\n",
       " ')',\n",
       " '将',\n",
       " '(',\n",
       " '都',\n",
       " '资',\n",
       " 'e',\n",
       " '6',\n",
       " '基',\n",
       " '用',\n",
       " '面',\n",
       " '产',\n",
       " '还',\n",
       " '自',\n",
       " '者',\n",
       " '本',\n",
       " '之',\n",
       " '美',\n",
       " '很',\n",
       " '同',\n",
       " '',\n",
       " '7',\n",
       " '部',\n",
       " '进',\n",
       " '但',\n",
       " '主',\n",
       " '外',\n",
       " '动',\n",
       " '机',\n",
       " '元',\n",
       " '理',\n",
       " '加',\n",
       " 'a',\n",
       " '全',\n",
       " '与',\n",
       " '实',\n",
       " '影',\n",
       " '好',\n",
       " '小',\n",
       " '间',\n",
       " '其',\n",
       " '天',\n",
       " '定',\n",
       " '表',\n",
       " '力',\n",
       " '如',\n",
       " '次',\n",
       " '合',\n",
       " '长',\n",
       " 'o',\n",
       " '体',\n",
       " '价',\n",
       " 'i',\n",
       " '所',\n",
       " '内',\n",
       " '子',\n",
       " '目',\n",
       " '电',\n",
       " '-',\n",
       " '当',\n",
       " '度',\n",
       " '品',\n",
       " '看',\n",
       " '期',\n",
       " '关',\n",
       " '更',\n",
       " 'n',\n",
       " '等',\n",
       " '工',\n",
       " '然',\n",
       " '斯',\n",
       " '重',\n",
       " '些',\n",
       " '球',\n",
       " '此',\n",
       " '里',\n",
       " '利',\n",
       " '相',\n",
       " '情',\n",
       " '投',\n",
       " '点',\n",
       " '没',\n",
       " '因',\n",
       " '已',\n",
       " '三',\n",
       " '心',\n",
       " '特',\n",
       " '明',\n",
       " '位',\n",
       " '两',\n",
       " '法',\n",
       " '从',\n",
       " '入',\n",
       " '名',\n",
       " '万',\n",
       " '手',\n",
       " '计',\n",
       " '性',\n",
       " '事',\n",
       " '只',\n",
       " '样',\n",
       " '示',\n",
       " 'r',\n",
       " '种',\n",
       " '报',\n",
       " '海',\n",
       " '平',\n",
       " '数',\n",
       " '%',\n",
       " '第',\n",
       " '并',\n",
       " '色',\n",
       " '建',\n",
       " '据',\n",
       " '提',\n",
       " '商',\n",
       " '员',\n",
       " '通',\n",
       " '去',\n",
       " '民',\n",
       " 't',\n",
       " '着',\n",
       " '你',\n",
       " '片',\n",
       " '展',\n",
       " '道',\n",
       " '文',\n",
       " '演',\n",
       " '赛',\n",
       " '区',\n",
       " '交',\n",
       " '意',\n",
       " '政',\n",
       " '么',\n",
       " '今',\n",
       " '让',\n",
       " '起',\n",
       " '信',\n",
       " '化',\n",
       " '银',\n",
       " '记',\n",
       " '司',\n",
       " '北',\n",
       " '游',\n",
       " '科',\n",
       " '戏',\n",
       " '被',\n",
       " '格',\n",
       " '保',\n",
       " '及',\n",
       " '常',\n",
       " '物',\n",
       " '问',\n",
       " 's',\n",
       " '量',\n",
       " '制',\n",
       " '持',\n",
       " '果',\n",
       " '感',\n",
       " '设',\n",
       " '队',\n",
       " '无',\n",
       " '收',\n",
       " '正',\n",
       " '应',\n",
       " '？',\n",
       " '那',\n",
       " '活',\n",
       " '身',\n",
       " '式',\n",
       " '打',\n",
       " 'l',\n",
       " '系',\n",
       " '尔',\n",
       " '总',\n",
       " '京',\n",
       " 'A',\n",
       " '至',\n",
       " '己',\n",
       " '务',\n",
       " '受',\n",
       " '想',\n",
       " '星',\n",
       " '回',\n",
       " '留',\n",
       " '由',\n",
       " '网',\n",
       " '达',\n",
       " '认',\n",
       " '做',\n",
       " '题',\n",
       " '选',\n",
       " '费',\n",
       " '城',\n",
       " '增',\n",
       " '近',\n",
       " '华',\n",
       " '风',\n",
       " '非',\n",
       " '战',\n",
       " '布',\n",
       " '该',\n",
       " '接',\n",
       " '款',\n",
       " '项',\n",
       " 'S',\n",
       " '英',\n",
       " '女',\n",
       " '程',\n",
       " '导',\n",
       " '管',\n",
       " '二',\n",
       " '强',\n",
       " '证',\n",
       " '水',\n",
       " '代',\n",
       " '调',\n",
       " '少',\n",
       " '专',\n",
       " '型',\n",
       " '台',\n",
       " '给',\n",
       " '需',\n",
       " '规',\n",
       " '图',\n",
       " '周',\n",
       " '德',\n",
       " '解',\n",
       " '各',\n",
       " 'C',\n",
       " '向',\n",
       " '；',\n",
       " '别',\n",
       " '股',\n",
       " '东',\n",
       " '结',\n",
       " '或',\n",
       " '首',\n",
       " '士',\n",
       " '西',\n",
       " '安',\n",
       " '教',\n",
       " '变',\n",
       " '火',\n",
       " '节',\n",
       " '际',\n",
       " '任',\n",
       " '单',\n",
       " '先',\n",
       " '再',\n",
       " '观',\n",
       " '校',\n",
       " '显',\n",
       " '克',\n",
       " '组',\n",
       " '直',\n",
       " '装',\n",
       " '求',\n",
       " '才',\n",
       " '率',\n",
       " '较',\n",
       " '造',\n",
       " '使',\n",
       " '每',\n",
       " '考',\n",
       " '亿',\n",
       " '请',\n",
       " '流',\n",
       " '完',\n",
       " '拍',\n",
       " '取',\n",
       " '院',\n",
       " '线',\n",
       " '门',\n",
       " '放',\n",
       " '世',\n",
       " '住',\n",
       " '续',\n",
       " '联',\n",
       " '张',\n",
       " 'P',\n",
       " '息',\n",
       " '空',\n",
       " 'c',\n",
       " '研',\n",
       " '它',\n",
       " '十',\n",
       " '立',\n",
       " '原',\n",
       " '易',\n",
       " '整',\n",
       " '—',\n",
       " '术',\n",
       " '论',\n",
       " '消',\n",
       " '太',\n",
       " '知',\n",
       " '王',\n",
       " '质',\n",
       " '约',\n",
       " '客',\n",
       " '几',\n",
       " '配',\n",
       " '马',\n",
       " '统',\n",
       " '值',\n",
       " 'T',\n",
       " '头',\n",
       " '件',\n",
       " '带',\n",
       " '服',\n",
       " '她',\n",
       " '界',\n",
       " '指',\n",
       " 'D',\n",
       " '级',\n",
       " '企',\n",
       " '传',\n",
       " '老',\n",
       " '类',\n",
       " '始',\n",
       " '气',\n",
       " 'I',\n",
       " '户',\n",
       " '超',\n",
       " '育',\n",
       " '未',\n",
       " '具',\n",
       " 'm',\n",
       " '济',\n",
       " '低',\n",
       " '处',\n",
       " '技',\n",
       " '望',\n",
       " '把',\n",
       " 'M',\n",
       " '究',\n",
       " '什',\n",
       " '热',\n",
       " '推',\n",
       " '称',\n",
       " '购',\n",
       " '！',\n",
       " '季',\n",
       " '米',\n",
       " '光',\n",
       " '广',\n",
       " '份',\n",
       " '乐',\n",
       " '又',\n",
       " '获',\n",
       " '真',\n",
       " '觉',\n",
       " 'E',\n",
       " '玩',\n",
       " '形',\n",
       " '集',\n",
       " '备',\n",
       " '优',\n",
       " '领',\n",
       " '势',\n",
       " 'h',\n",
       " '楼',\n",
       " '准',\n",
       " '包',\n",
       " '像',\n",
       " '则',\n",
       " '难',\n",
       " '爱',\n",
       " '四',\n",
       " '申',\n",
       " 'B',\n",
       " 'd',\n",
       " '售',\n",
       " 'u',\n",
       " '快',\n",
       " '连',\n",
       " '话',\n",
       " '师',\n",
       " '告',\n",
       " '何',\n",
       " '视',\n",
       " '确',\n",
       " '深',\n",
       " '预',\n",
       " '局',\n",
       " '亚',\n",
       " '评',\n",
       " '票',\n",
       " '卡',\n",
       " '改',\n",
       " '候',\n",
       " '买',\n",
       " '况',\n",
       " '存',\n",
       " '支',\n",
       " '拉',\n",
       " '仅',\n",
       " '办',\n",
       " '注',\n",
       " '反',\n",
       " '环',\n",
       " '供',\n",
       " '角',\n",
       " '篮',\n",
       " '标',\n",
       " '转',\n",
       " '拿',\n",
       " '运',\n",
       " '参',\n",
       " '升',\n",
       " '众',\n",
       " 'O',\n",
       " '/',\n",
       " '创',\n",
       " '浪',\n",
       " '陈',\n",
       " '精',\n",
       " '号',\n",
       " '见',\n",
       " '助',\n",
       " '且',\n",
       " '土',\n",
       " '半',\n",
       " '议',\n",
       " '花',\n",
       " '步',\n",
       " '策',\n",
       " '南',\n",
       " '板',\n",
       " '居',\n",
       " '友',\n",
       " '越',\n",
       " '功',\n",
       " '构',\n",
       " '贷',\n",
       " '划',\n",
       " '模',\n",
       " '山',\n",
       " '融',\n",
       " '社',\n",
       " 'G',\n",
       " '决',\n",
       " '共',\n",
       " '案',\n",
       " 'g',\n",
       " '销',\n",
       " '走',\n",
       " '套',\n",
       " '路',\n",
       " '即',\n",
       " '言',\n",
       " '容',\n",
       " '均',\n",
       " '试',\n",
       " '积',\n",
       " '团',\n",
       " '府',\n",
       " '查',\n",
       " '条',\n",
       " '限',\n",
       " '引',\n",
       " '随',\n",
       " '择',\n",
       " '希',\n",
       " '口',\n",
       " '奇',\n",
       " '涨',\n",
       " '港',\n",
       " '盘',\n",
       " '象',\n",
       " '置',\n",
       " '林',\n",
       " '益',\n",
       " '效',\n",
       " '神',\n",
       " '极',\n",
       " '语',\n",
       " '复',\n",
       " '够',\n",
       " '阿',\n",
       " '币',\n",
       " '根',\n",
       " '州',\n",
       " '历',\n",
       " '采',\n",
       " '奖',\n",
       " '终',\n",
       " '适',\n",
       " 'N',\n",
       " '尼',\n",
       " '命',\n",
       " '段',\n",
       " '景',\n",
       " '险',\n",
       " '却',\n",
       " '失',\n",
       " '军',\n",
       " '权',\n",
       " '车',\n",
       " 'L',\n",
       " '剧',\n",
       " '速',\n",
       " '攻',\n",
       " '许',\n",
       " '足',\n",
       " '算',\n",
       " '清',\n",
       " '争',\n",
       " '响',\n",
       " '移',\n",
       " '红',\n",
       " '必',\n",
       " '幅',\n",
       " '额',\n",
       " 'y',\n",
       " '讯',\n",
       " '照',\n",
       " '源',\n",
       " '李',\n",
       " '验',\n",
       " '双',\n",
       " '刚',\n",
       " '另',\n",
       " '态',\n",
       " 'p',\n",
       " '致',\n",
       " '底',\n",
       " '除',\n",
       " '满',\n",
       " '击',\n",
       " '白',\n",
       " '五',\n",
       " '往',\n",
       " '喜',\n",
       " '财',\n",
       " '防',\n",
       " '境',\n",
       " '排',\n",
       " '百',\n",
       " '降',\n",
       " '远',\n",
       " '书',\n",
       " '习',\n",
       " '龙',\n",
       " '债',\n",
       " '钱',\n",
       " '断',\n",
       " '富',\n",
       " '曾',\n",
       " '测',\n",
       " '签',\n",
       " '举',\n",
       " '飞',\n",
       " '便',\n",
       " '介',\n",
       " '博',\n",
       " '牌',\n",
       " '虽',\n",
       " '料',\n",
       " '例',\n",
       " '边',\n",
       " '罗',\n",
       " '搭',\n",
       " '香',\n",
       " '委',\n",
       " '营',\n",
       " '器',\n",
       " 'R',\n",
       " '落',\n",
       " '跟',\n",
       " '列',\n",
       " '仍',\n",
       " '职',\n",
       " '尚',\n",
       " '阳',\n",
       " '监',\n",
       " '券',\n",
       " '儿',\n",
       " '尽',\n",
       " '衣',\n",
       " '负',\n",
       " '短',\n",
       " '找',\n",
       " '层',\n",
       " '黄',\n",
       " '艺',\n",
       " '域',\n",
       " '施',\n",
       " 'F',\n",
       " '离',\n",
       " '波',\n",
       " '胜',\n",
       " '兰',\n",
       " '古',\n",
       " '录',\n",
       " '黑',\n",
       " '素',\n",
       " '普',\n",
       " '史',\n",
       " '版',\n",
       " '摄',\n",
       " '官',\n",
       " '否',\n",
       " '石',\n",
       " '·',\n",
       " '站',\n",
       " 'w',\n",
       " '访',\n",
       " ',',\n",
       " '初',\n",
       " '细',\n",
       " '继',\n",
       " '责',\n",
       " '括',\n",
       " '卫',\n",
       " '露',\n",
       " '协',\n",
       " '轻',\n",
       " '维',\n",
       " '澳',\n",
       " '纪',\n",
       " '欢',\n",
       " '卖',\n",
       " '早',\n",
       " '绩',\n",
       " '穿',\n",
       " '孩',\n",
       " '按',\n",
       " '眼',\n",
       " '围',\n",
       " '男',\n",
       " '依',\n",
       " '宝',\n",
       " '控',\n",
       " 'b',\n",
       " '读',\n",
       " '状',\n",
       " '洲',\n",
       " '密',\n",
       " '差',\n",
       " '奥',\n",
       " '破',\n",
       " '块',\n",
       " '突',\n",
       " '…',\n",
       " '思',\n",
       " '画',\n",
       " '巴',\n",
       " '纳',\n",
       " '范',\n",
       " '映',\n",
       " '充',\n",
       " '媒',\n",
       " '室',\n",
       " 'H',\n",
       " '材',\n",
       " '字',\n",
       " '武',\n",
       " '跌',\n",
       " '春',\n",
       " '湖',\n",
       " '简',\n",
       " '担',\n",
       " '欧',\n",
       " '谈',\n",
       " '切',\n",
       " '兴',\n",
       " '刘',\n",
       " '彩',\n",
       " 'V',\n",
       " '稳',\n",
       " '待',\n",
       " '箭',\n",
       " '属',\n",
       " '声',\n",
       " '课',\n",
       " '透',\n",
       " '紧',\n",
       " '温',\n",
       " '绝',\n",
       " '园',\n",
       " '修',\n",
       " '农',\n",
       " '严',\n",
       " '减',\n",
       " '江',\n",
       " '讲',\n",
       " '航',\n",
       " '似',\n",
       " '独',\n",
       " '背',\n",
       " '夫',\n",
       " '甚',\n",
       " '志',\n",
       " '牛',\n",
       " '亮',\n",
       " '故',\n",
       " 'W',\n",
       " 'f',\n",
       " '汇',\n",
       " '义',\n",
       " '换',\n",
       " '货',\n",
       " '令',\n",
       " '饰',\n",
       " '招',\n",
       " '医',\n",
       " '占',\n",
       " '音',\n",
       " '群',\n",
       " '竞',\n",
       " '乎',\n",
       " '护',\n",
       " '姆',\n",
       " '席',\n",
       " '湾',\n",
       " '略',\n",
       " '央',\n",
       " '怎',\n",
       " '岁',\n",
       " '亲',\n",
       " '送',\n",
       " '补',\n",
       " '青',\n",
       " '索',\n",
       " '批',\n",
       " '核',\n",
       " '帮',\n",
       " '福',\n",
       " '伤',\n",
       " 'v',\n",
       " '昨',\n",
       " '念',\n",
       " '激',\n",
       " '陆',\n",
       " '危',\n",
       " '巨',\n",
       " '省',\n",
       " '宣',\n",
       " '般',\n",
       " '筑',\n",
       " 'k',\n",
       " '识',\n",
       " '守',\n",
       " '压',\n",
       " '错',\n",
       " '免',\n",
       " '店',\n",
       " '宅',\n",
       " '庭',\n",
       " '承',\n",
       " '雷',\n",
       " '云',\n",
       " '味',\n",
       " '典',\n",
       " '听',\n",
       " '笑',\n",
       " '登',\n",
       " '假',\n",
       " '审',\n",
       " '绍',\n",
       " '毕',\n",
       " '诉',\n",
       " '迷',\n",
       " '威',\n",
       " '谢',\n",
       " '练',\n",
       " '丽',\n",
       " '止',\n",
       " '微',\n",
       " '频',\n",
       " '净',\n",
       " '森',\n",
       " '拥',\n",
       " '须',\n",
       " '左',\n",
       " '印',\n",
       " '追',\n",
       " '秀',\n",
       " '托',\n",
       " '吴',\n",
       " '副',\n",
       " '疑',\n",
       " '千',\n",
       " '治',\n",
       " '冰',\n",
       " '右',\n",
       " '松',\n",
       " '晚',\n",
       " '皮',\n",
       " 'K',\n",
       " '佳',\n",
       " '夏',\n",
       " '呢',\n",
       " '魔',\n",
       " '缺',\n",
       " '编',\n",
       " '析',\n",
       " '钟',\n",
       " '刻',\n",
       " '遇',\n",
       " '停',\n",
       " '写',\n",
       " '付',\n",
       " '食',\n",
       " '织',\n",
       " '闻',\n",
       " '检',\n",
       " '礼',\n",
       " '惊',\n",
       " '莱',\n",
       " '余',\n",
       " '死',\n",
       " '雅',\n",
       " '善',\n",
       " '干',\n",
       " '愿',\n",
       " '爆',\n",
       " '某',\n",
       " '端',\n",
       " '探',\n",
       " '述',\n",
       " '吧',\n",
       " '执',\n",
       " '村',\n",
       " '租',\n",
       " '健',\n",
       " '阵',\n",
       " '律',\n",
       " '虑',\n",
       " '临',\n",
       " '冲',\n",
       " '盟',\n",
       " '六',\n",
       " '吸',\n",
       " '束',\n",
       " '射',\n",
       " '久',\n",
       " '馆',\n",
       " '暴',\n",
       " '折',\n",
       " '苏',\n",
       " '顿',\n",
       " '借',\n",
       " '宇',\n",
       " 'U',\n",
       " '病',\n",
       " '盛',\n",
       " '藏',\n",
       " '杰',\n",
       " '灵',\n",
       " '异',\n",
       " '察',\n",
       " '震',\n",
       " '退',\n",
       " '朋',\n",
       " '旅',\n",
       " '抢',\n",
       " '韩',\n",
       " '封',\n",
       " '轮',\n",
       " '康',\n",
       " '弹',\n",
       " '互',\n",
       " '伦',\n",
       " '败',\n",
       " '午',\n",
       " '河',\n",
       " '杀',\n",
       " '私',\n",
       " '镜',\n",
       " '扁',\n",
       " '潮',\n",
       " '幕',\n",
       " '旗',\n",
       " '党',\n",
       " '休',\n",
       " '八',\n",
       " '悉',\n",
       " '瑞',\n",
       " '逐',\n",
       " '征',\n",
       " '坚',\n",
       " '绿',\n",
       " '班',\n",
       " '迎',\n",
       " '吃',\n",
       " '母',\n",
       " '族',\n",
       " '税',\n",
       " '良',\n",
       " '丰',\n",
       " '训',\n",
       " '秘',\n",
       " '娱',\n",
       " '哈',\n",
       " '警',\n",
       " '伟',\n",
       " '您',\n",
       " '歌',\n",
       " '烈',\n",
       " '哪',\n",
       " '挑',\n",
       " '享',\n",
       " '肯',\n",
       " '凯',\n",
       " '顾',\n",
       " '杨',\n",
       " '丹',\n",
       " '雄',\n",
       " '键',\n",
       " '靠',\n",
       " '针',\n",
       " '木',\n",
       " '估',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练文本中所有出现过的字及其所对应的id\n",
    "words, word_to_id = read_vocab('cnews.vocab.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1609,  659,   56, ...,    9,  311,    3],\n",
       "       [   2,  101,   16, ..., 1168,    3,   24],\n",
       "       [ 465,  855,  521, ...,  116,  136,   85],\n",
       "       ...,\n",
       "       [  49,   18,   79, ...,  836, 1928, 1072],\n",
       "       [ 166,  110,  714, ...,  836, 1928, 1072],\n",
       "       [   1,   80,  551, ...,   78,  192,    3]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练数据每个字的id和对应标签的one-hot形式\n",
    "x_train, y_train = process_file(train_file, word_to_id, cat_to_id, 600)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = process_file(val_file, word_to_id, cat_to_id, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "# 设置GPU\n",
    "cuda = torch.device('cuda')\n",
    "x_train, y_train = torch.LongTensor(x_train), torch.Tensor(y_train)\n",
    "x_val, y_val = torch.LongTensor(x_val), torch.Tensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数设置\n",
    "Batch_Size = 256\n",
    "Train_Epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(x_train, y_train)\n",
    "train_loader = Data.DataLoader(dataset=train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "val_dataset = Data.TensorDataset(x_val, y_val)\n",
    "val_loader = Data.DataLoader(dataset=val_dataset, batch_size=Batch_Size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wll/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss= tensor(0.7221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.275\n",
      "epoch= 1\n",
      "train loss= tensor(0.7165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.375\n",
      "epoch= 2\n",
      "train loss= tensor(0.7067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4625\n",
      "epoch= 3\n",
      "train loss= tensor(0.7133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3625\n",
      "epoch= 4\n",
      "train loss= tensor(0.7180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3375\n",
      "model.pkl saved\n",
      "val accuracy: 0.90234375\n",
      "epoch= 5\n",
      "train loss= tensor(0.7167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.325\n",
      "epoch= 6\n",
      "train loss= tensor(0.7098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4\n",
      "epoch= 7\n",
      "train loss= tensor(0.7126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3625\n",
      "epoch= 8\n",
      "train loss= tensor(0.7178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3375\n",
      "epoch= 9\n",
      "train loss= tensor(0.7201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3\n",
      "epoch= 10\n",
      "train loss= tensor(0.7031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4875\n",
      "epoch= 11\n",
      "train loss= tensor(0.7102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.325\n",
      "epoch= 12\n",
      "train loss= tensor(0.7120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3875\n",
      "epoch= 13\n",
      "train loss= tensor(0.7081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4375\n",
      "epoch= 14\n",
      "train loss= tensor(0.7126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.375\n",
      "epoch= 15\n",
      "train loss= tensor(0.7163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3625\n",
      "epoch= 16\n",
      "train loss= tensor(0.6997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.55\n",
      "epoch= 17\n",
      "train loss= tensor(0.7107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4125\n",
      "epoch= 18\n",
      "train loss= tensor(0.7103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.425\n",
      "epoch= 19\n",
      "train loss= tensor(0.7014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.5\n",
      "epoch= 20\n",
      "train loss= tensor(0.7081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.425\n",
      "epoch= 21\n",
      "train loss= tensor(0.7188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3375\n",
      "epoch= 22\n",
      "train loss= tensor(0.7209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3125\n",
      "epoch= 23\n",
      "train loss= tensor(0.7077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4375\n",
      "epoch= 24\n",
      "train loss= tensor(0.7077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.5\n",
      "epoch= 25\n",
      "train loss= tensor(0.7138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.375\n",
      "epoch= 26\n",
      "train loss= tensor(0.7178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.35\n",
      "epoch= 27\n",
      "train loss= tensor(0.7101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3875\n",
      "epoch= 28\n",
      "train loss= tensor(0.7108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.425\n",
      "epoch= 29\n",
      "train loss= tensor(0.7052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.5\n",
      "epoch= 30\n",
      "train loss= tensor(0.7125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4\n",
      "epoch= 31\n",
      "train loss= tensor(0.7163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.35\n",
      "epoch= 32\n",
      "train loss= tensor(0.7101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4375\n",
      "epoch= 33\n",
      "train loss= tensor(0.7090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.375\n",
      "epoch= 34\n",
      "train loss= tensor(0.7053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4625\n",
      "epoch= 35\n",
      "train loss= tensor(0.7070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4125\n",
      "epoch= 36\n",
      "train loss= tensor(0.7163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.4\n",
      "epoch= 37\n",
      "train loss= tensor(0.7161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3375\n",
      "epoch= 38\n",
      "train loss= tensor(0.7089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.425\n",
      "epoch= 39\n",
      "train loss= tensor(0.7140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "train accuracy: 0.3875\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "model = train(Train_Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
