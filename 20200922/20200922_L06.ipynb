{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核心能力提升班商业智能方向 004期 Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 1: XGBoost与GBDT的区别是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost是GBDT在工程上的实现，主要的区别在目标函数上加入了正则化项，并且对目标函数进行了二阶泰勒展开并保留了二次项，正则化项中包括两项一个是树里面叶子节点的个数，一个是树上叶子节点的得分的L2模平方。XGBoost在分裂节点也进行了优化，采用了贪心算法，不计算两两之间的组合而是按照某个特征值的大小进行排序，进行分割。对于数据量较大的情况，采用分桶方法大幅度减小计算量（Histogram）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 2: 举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何... 请分享到课程微信群中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我之前做过二手手机价值预测，需要根据二手手机的内存、存储容量、购买渠道等基本属性和机身外观、屏幕外观、屏幕显示、拆修情况等折损属性来预测二手手机的回收价值。我用的是两个GBDT模型分别对基本属性和折损属性进行特征提取，之后使用LR进行价值预测。  \n",
    "在课上做员工离职预测时使用的是LR模型，我的做法是先将特征按照离散值和连续值进行了分类，然后离散值使用OneHot编码，连续值使用Z-Score规范化进行处理，将两种特征进行拼接之后使用LR进行员工离职率进行预测。目前是第七名的位置。<img src=\"./rank.PNG\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking 3： 请你思考，在你的工作中，需要构建哪些特征（比如用户画像，item特征...），这些特征都包括哪些维度（鼓励分享到微信群中，进行交流）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在目前的工作中，在做序列推荐任务，需要根据用户点击或购买的历史数据推断用户的购买习惯或兴趣，预测用户接下来感兴趣的商品。需要构建的特征包括用户特征：性别、年龄段、工作、邮政编码，用户点击item的时间和时间戳，还有item的属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action 1： 男女声音识别\n",
    "数据集：voice.csv  \n",
    "3168个录制的声音样本（来自男性和女性演讲者），采集的频率范围是0hz-280hz，已经对数据进行了预处理  \n",
    "一共有21个属性值，请判断该声音是男还是女？  \n",
    "使用Accuracy作为评价标准  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/voice.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['meanfreq',\n",
       "  'sd',\n",
       "  'median',\n",
       "  'Q25',\n",
       "  'Q75',\n",
       "  'IQR',\n",
       "  'skew',\n",
       "  'kurt',\n",
       "  'sp.ent',\n",
       "  'sfm',\n",
       "  'mode',\n",
       "  'centroid',\n",
       "  'meanfun',\n",
       "  'minfun',\n",
       "  'maxfun',\n",
       "  'meandom',\n",
       "  'mindom',\n",
       "  'maxdom',\n",
       "  'dfrange',\n",
       "  'modindx'],\n",
       " 'label')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_label = data.columns.tolist()[:-1]\n",
    "target_label = data.columns.tolist()[-1]\n",
    "feature_label, target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[target_label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data[feature_label].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用Z-Score规范化\n",
    "ss = preprocessing.StandardScaler()\n",
    "feature = ss.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE = preprocessing.LabelEncoder()\n",
    "target = LE.fit_transform(target).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.04924806,  0.4273553 , -4.22490077, ..., -1.43142165,\n",
       "         -1.41913712, -1.45477229],\n",
       "        [-3.84105325,  0.6116695 , -3.99929342, ..., -1.41810716,\n",
       "         -1.4058184 , -1.01410294],\n",
       "        [-3.46306647,  1.60384791, -4.09585052, ..., -1.42920257,\n",
       "         -1.41691733, -1.06534356],\n",
       "        ...,\n",
       "        [-1.29877326,  2.32272355, -0.05197279, ..., -0.5992661 ,\n",
       "         -0.58671739,  0.17588664],\n",
       "        [-1.2452018 ,  2.012196  , -0.01772849, ..., -0.41286326,\n",
       "         -0.40025537,  1.14916112],\n",
       "        [-0.51474626,  2.14765111, -0.07087873, ..., -1.27608595,\n",
       "         -1.2637521 ,  1.47567886]]), array([1., 1., 1., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割数据，将20%的数据作为测试集，其余作为训练集\n",
    "train_x, test_x, train_y, test_y = train_test_split(feature, target, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM准确率: 0.9795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(train_x, train_y)\n",
    "predict_y = svc.predict(test_x)\n",
    "print('SVM准确率: %0.4lf' % accuracy_score(predict_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART准确率: 0.9574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "CART = DecisionTreeClassifier()\n",
    "CART.fit(train_x, train_y)\n",
    "predict_y = CART.predict(test_x)\n",
    "print('CART准确率: %0.4lf' % accuracy_score(predict_y, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归准确率: 0.9669\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogR = LogisticRegression(solver='lbfgs')\n",
    "LogR.fit(train_x, train_y)\n",
    "predict_y = LogR.predict(test_x)\n",
    "print('逻辑回归准确率: %0.4lf' % accuracy_score(predict_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF+LR进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再将训练集拆成两个部分（RF，LR）\n",
    "X_train, X_train_lr, y_train, y_train_lr = train_test_split(train_x, train_y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimator = 10\n",
    "# 基于随机森林的监督变换\n",
    "rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到OneHot编码\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf_enc.fit(rf.apply(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用OneHot编码作为特征，训练LR\n",
    "rf_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "rf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF+LR准确率: 0.9700\n"
     ]
    }
   ],
   "source": [
    "# 使用LR进行预测\n",
    "y_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(test_x)))[:, 1]\n",
    "print('RF+LR准确率: %0.4lf' % accuracy_score(y_pred_rf_lm.round(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT+LR进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT+LR准确率: 0.9653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "n_estimator = 10\n",
    "# 基于GBDT的监督变换\n",
    "gbdt = GradientBoostingClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "gbdt.fit(X_train, y_train)\n",
    "# 得到OneHot编码\n",
    "gbdt_enc = OneHotEncoder(categories='auto')\n",
    "gbdt_enc.fit(gbdt.apply(X_train).reshape(-1,10))\n",
    "# 使用OneHot编码作为特征，训练LR\n",
    "gbdt_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "gbdt_lm.fit(gbdt_enc.transform(gbdt.apply(X_train_lr).reshape(-1,10)), y_train_lr)\n",
    "# 使用LR进行预测\n",
    "y_pred_gbdt_lm = gbdt_lm.predict_proba(gbdt_enc.transform(gbdt.apply(test_x).reshape(-1,10)))[:, 1]\n",
    "print('GBDT+LR准确率: %0.4lf' % accuracy_score(y_pred_gbdt_lm.round(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'boosting_type':'gbdt',\n",
    "         'objective' : 'binary:logistic', #任务目标\n",
    "         'eval_metric' : 'auc', #评估指标\n",
    "         'eta' : 0.01, #学习率\n",
    "         'max_depth' : 15, #树最大深度\n",
    "         'colsample_bytree':0.8, #设置在每次迭代中使用特征的比例\n",
    "         'subsample': 0.9, #样本采样比例\n",
    "         'subsample_freq': 8, #bagging的次数\n",
    "         'alpha': 0.6, #L1正则\n",
    "         'lambda': 0, #L2正则\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = xgb.DMatrix(train_x, train_y)\n",
    "test_data = xgb.DMatrix(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:59:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { boosting_type, subsample_freq } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.99119\tvalid-auc:0.98693\n",
      "Multiple eval metrics have been passed: 'valid-auc' will be used for early stopping.\n",
      "\n",
      "Will train until valid-auc hasn't improved in 25 rounds.\n",
      "[25]\ttrain-auc:0.99873\tvalid-auc:0.99568\n",
      "[50]\ttrain-auc:0.99884\tvalid-auc:0.99614\n",
      "Stopping. Best iteration:\n",
      "[28]\ttrain-auc:0.99878\tvalid-auc:0.99649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, train_data, evals=[(train_data, 'train'), (test_data, 'valid')], num_boost_round=300, early_stopping_rounds=25, verbose_eval=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost准确率: 0.9716\n"
     ]
    }
   ],
   "source": [
    "predict_y = model.predict(test_data)\n",
    "print('XGBoost准确率: %0.4lf' % accuracy_score(predict_y.round(), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnBI",
   "language": "python",
   "name": "learnbi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
